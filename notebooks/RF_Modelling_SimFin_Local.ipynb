{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "691ffe59",
   "metadata": {},
   "source": [
    "# Random Forest modelling (SimFin local) \u2014 paper-faithful walk-forward backtest (improved)\n",
    "\n",
    "This notebook refactors `RF_Modelling_SimFin_Local_v3.ipynb` to better reproduce the methodology in **Morgan Wynne (2023)**:\n",
    "\n",
    "- **Investment date:** last month-end of each year (typically **31/12/YYYY**).\n",
    "- **No look-ahead:** for an investment date `D`, train uses data **up to 31/12/(Y-1)** (one year before `D`), so training targets (1-year forward returns) cannot peek beyond `D`.\n",
    "- **Walk-forward evaluation:** repeat annually across decades (first test year defaults to **min_year + 11**, matching \u201c10 years of history\u201d).\n",
    "- **Time-respecting hyperparameter tuning:** optional **purged, date-based CV** (no random shuffling).\n",
    "- **Missing data handling:** offers a paper-faithful **drop strategy** *and* safer imputation alternatives (so you can see the trade-offs explicitly).\n",
    "\n",
    "> Tip: keep `MISSING_STRATEGY=\"paper_drop\"` and `RF_PARAMS_PAPER` for the closest reproduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8ddd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core deps\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import warnings\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Dict, Optional, Tuple, Iterable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import ParameterGrid, ParameterSampler\n",
    "\n",
    "from scipy.stats import ttest_ind, spearmanr\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e37971",
   "metadata": {},
   "source": [
    "## 1) Configuration\n",
    "\n",
    "Keep all \u201cmagic numbers\u201d here so the experiment is fully reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2157765e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Config:\n",
    "    # ---- Data ----\n",
    "    PANEL_PATH: str = \"data/simfin_panel.csv\"\n",
    "    DATE_COL: str = \"public_date\"\n",
    "    ID_COL: str = \"TICKER\"\n",
    "    TARGET_COL: str = \"1yr_return\"\n",
    "\n",
    "    # ---- Universe controls ----\n",
    "    CAP_COL: str = \"cap\"                  # optional (ignored if missing)\n",
    "    ALLOW_CAPS: Optional[List[str]] = None  # e.g. [\"Mid Cap\",\"Large Cap\",\"Mega Cap\"]\n",
    "\n",
    "    # ---- Backtest setup (paper-faithful) ----\n",
    "    # First investment date defaults to min_year + 11 (10y history, then first invest at year-end).\n",
    "    FIRST_TEST_YEAR: Optional[int] = None\n",
    "    LAST_TEST_YEAR: Optional[int] = None  # inclusive; default auto based on data coverage\n",
    "\n",
    "    # For investment date D in year Y:\n",
    "    # train_end = Dec 31 of (Y-1)  (paper: \u201cone year prior\u201d to avoid target peeking)\n",
    "    PURGE_MONTHS: int = 12\n",
    "\n",
    "    # ---- Feature selection ----\n",
    "    FEATURE_MODE: str = \"auto_numeric\"  # \"auto_numeric\" | \"manual\"\n",
    "    MANUAL_FEATURES: Optional[List[str]] = None\n",
    "\n",
    "    # ---- Missing data handling ----\n",
    "    # \"paper_drop\": drop features with too much missingness (based on training only),\n",
    "    #               then drop any rows with missing among remaining features.\n",
    "    # \"median_impute\": median impute (fit on training only), optionally add missing indicators\n",
    "    # \"ffill_then_median\": per-ticker forward-fill then median impute\n",
    "    MISSING_STRATEGY: str = \"paper_drop\"\n",
    "    ADD_MISSING_INDICATORS: bool = True  # for imputation strategies\n",
    "\n",
    "    # Paper drops features with >120k missing out of ~2.4M rows (~5%).\n",
    "    # We express this as a rate, applied per *training window*.\n",
    "    MAX_FEATURE_MISSING_RATE: float = 0.05\n",
    "\n",
    "    # ---- Model params ----\n",
    "    # Closest to the thesis:\n",
    "    # - max_features = sqrt(#features)\n",
    "    # - bootstrap sample size per tree = 10%\n",
    "    RF_PARAMS_PAPER: Dict = None\n",
    "\n",
    "    # ---- Portfolio sizes ----\n",
    "    PORTFOLIO_SIZES: Tuple[int, ...] = (30, 50, 100, 200)\n",
    "\n",
    "    # ---- Outputs ----\n",
    "    OUT_DIR: str = \"outputs/rf_simfin_local_v4\"\n",
    "\n",
    "def default_rf_params_paper():\n",
    "    return dict(\n",
    "        n_estimators=350,\n",
    "        max_depth=13,\n",
    "        max_features=\"sqrt\",\n",
    "        bootstrap=True,\n",
    "        max_samples=0.10,     # 10% bootstrap sample per tree (paper)\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "CFG = Config(RF_PARAMS_PAPER=default_rf_params_paper())\n",
    "\n",
    "os.makedirs(CFG.OUT_DIR, exist_ok=True)\n",
    "print(\"OUT_DIR:\", CFG.OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb31093e",
   "metadata": {},
   "source": [
    "## 2) Load data (no global target filtering)\n",
    "\n",
    "Important: we **do not** drop missing targets globally, to avoid changing the sample composition for earlier years.\n",
    "Targets are filtered **inside each walk-forward iteration**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcaa35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_panel(cfg: Config) -> pd.DataFrame:\n",
    "    df = pd.read_csv(cfg.PANEL_PATH)\n",
    "    df[cfg.DATE_COL] = pd.to_datetime(df[cfg.DATE_COL], errors=\"coerce\")\n",
    "\n",
    "    # Basic hygiene\n",
    "    df = df.dropna(subset=[cfg.DATE_COL]).copy()\n",
    "    df = df.sort_values([cfg.DATE_COL, cfg.ID_COL]).reset_index(drop=True)\n",
    "\n",
    "    # Ensure target is numeric but do NOT drop NAs here\n",
    "    df[cfg.TARGET_COL] = pd.to_numeric(df[cfg.TARGET_COL], errors=\"coerce\")\n",
    "    df[cfg.TARGET_COL] = df[cfg.TARGET_COL].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = load_panel(CFG)\n",
    "print(\"Rows:\", len(df))\n",
    "print(\"Date range:\", df[CFG.DATE_COL].min(), \"->\", df[CFG.DATE_COL].max())\n",
    "print(\"Columns:\", len(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f85b157",
   "metadata": {},
   "source": [
    "## 3) Feature set\n",
    "\n",
    "To match the thesis most closely, you typically want all available **numeric** fundamental/macro features, excluding identifiers and the target.\n",
    "You can switch to `FEATURE_MODE=\"manual\"` if you have a fixed \u201cpaper feature list\u201d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393102a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NON_FEATURE_COLS = {CFG.DATE_COL, CFG.ID_COL, CFG.TARGET_COL, \"gvkey\", \"public_date\", \"Fiscal Year\", \"Fiscal Period\", \"Publish Date\"}\n",
    "\n",
    "def infer_features(df: pd.DataFrame, cfg: Config) -> List[str]:\n",
    "    if cfg.FEATURE_MODE == \"manual\":\n",
    "        if not cfg.MANUAL_FEATURES:\n",
    "            raise ValueError(\"FEATURE_MODE='manual' but MANUAL_FEATURES is empty.\")\n",
    "        feats = [c for c in cfg.MANUAL_FEATURES if c in df.columns]\n",
    "        missing = set(cfg.MANUAL_FEATURES) - set(feats)\n",
    "        if missing:\n",
    "            print(\"[WARN] Missing manual features:\", sorted(missing)[:20], \"...\")\n",
    "        return feats\n",
    "\n",
    "    # auto_numeric\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    feats = [c for c in numeric_cols if c not in NON_FEATURE_COLS]\n",
    "    return feats\n",
    "\n",
    "ALL_FEATURES = infer_features(df, CFG)\n",
    "print(\"Inferred feature count:\", len(ALL_FEATURES))\n",
    "print(\"Example features:\", ALL_FEATURES[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c08a3bd",
   "metadata": {},
   "source": [
    "## 4) Missingness diagnostics (global)\n",
    "\n",
    "This does **not** drive feature selection (that happens per training window), but it helps you understand where missingness comes from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8c3554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_report(df: pd.DataFrame, features: List[str], top_n: int = 30) -> pd.DataFrame:\n",
    "    miss = df[features].isna().mean().sort_values(ascending=False)\n",
    "    rep = pd.DataFrame({\"missing_rate\": miss, \"missing_count\": (miss * len(df)).round().astype(int)})\n",
    "    return rep.head(top_n)\n",
    "\n",
    "miss_top = missing_report(df, ALL_FEATURES, top_n=30)\n",
    "display(miss_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71de941",
   "metadata": {},
   "source": [
    "## 5) Date utilities and paper-faithful investment schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6186d738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_month_end_in_year(df: pd.DataFrame, date_col: str, year: int) -> Optional[pd.Timestamp]:\n",
    "    s = df.loc[df[date_col].dt.year == year, date_col]\n",
    "    if len(s) == 0:\n",
    "        return None\n",
    "    return pd.to_datetime(s.max())\n",
    "\n",
    "def year_end(year: int) -> pd.Timestamp:\n",
    "    return pd.Timestamp(year=year, month=12, day=31)\n",
    "\n",
    "def auto_test_year_range(df: pd.DataFrame, cfg: Config) -> Tuple[int, int]:\n",
    "    min_year = int(df[cfg.DATE_COL].dt.year.min())\n",
    "    max_year = int(df[cfg.DATE_COL].dt.year.max())\n",
    "\n",
    "    first = cfg.FIRST_TEST_YEAR if cfg.FIRST_TEST_YEAR is not None else (min_year + 11)\n",
    "    # Need at least one more year after investment date to have 1yr_return observed in test set\n",
    "    last = cfg.LAST_TEST_YEAR if cfg.LAST_TEST_YEAR is not None else (max_year - 1)\n",
    "    return first, last\n",
    "\n",
    "FIRST_Y, LAST_Y = auto_test_year_range(df, CFG)\n",
    "print(\"Test years:\", FIRST_Y, \"->\", LAST_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112b3345",
   "metadata": {},
   "source": [
    "## 6) Missing-data handling per window\n",
    "\n",
    "Two modes:\n",
    "- **paper_drop** (closest to thesis):  \n",
    "  1) drop features whose **training-window** missing rate exceeds `MAX_FEATURE_MISSING_RATE`,  \n",
    "  2) drop rows with any missing in remaining features (train and test separately),  \n",
    "  3) fit RF on complete-case.\n",
    "\n",
    "- **imputation modes**: safer for \u201creal\u201d trading pipelines, and useful to quantify selection bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1acaf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_universe_filter(frame: pd.DataFrame, cfg: Config) -> pd.DataFrame:\n",
    "    if cfg.ALLOW_CAPS is None:\n",
    "        return frame\n",
    "    if cfg.CAP_COL not in frame.columns:\n",
    "        print(\"[WARN] CAP_COL not present; ignoring ALLOW_CAPS.\")\n",
    "        return frame\n",
    "    return frame[frame[cfg.CAP_COL].isin(cfg.ALLOW_CAPS)].copy()\n",
    "\n",
    "def select_features_paper_drop(train_df: pd.DataFrame, features: List[str], cfg: Config) -> List[str]:\n",
    "    miss_rate = train_df[features].isna().mean()\n",
    "    keep = miss_rate[miss_rate <= cfg.MAX_FEATURE_MISSING_RATE].index.tolist()\n",
    "    # Also drop features that are entirely missing (can happen in short windows)\n",
    "    keep = [c for c in keep if train_df[c].notna().any()]\n",
    "    return keep\n",
    "\n",
    "def prep_Xy(\n",
    "    train_df: pd.DataFrame,\n",
    "    test_df: pd.DataFrame,\n",
    "    features: List[str],\n",
    "    cfg: Config,\n",
    "):\n",
    "    # Filter to rows where target is known in each set (NO global filtering)\n",
    "    train_df = train_df.dropna(subset=[cfg.TARGET_COL]).copy()\n",
    "    test_df_eval = test_df.dropna(subset=[cfg.TARGET_COL]).copy()\n",
    "\n",
    "    if cfg.MISSING_STRATEGY == \"paper_drop\":\n",
    "        keep_feats = select_features_paper_drop(train_df, features, cfg)\n",
    "\n",
    "        tr = train_df[keep_feats + [cfg.TARGET_COL]].dropna().copy()\n",
    "        te = test_df_eval[keep_feats + [cfg.TARGET_COL]].dropna().copy()\n",
    "\n",
    "        Xtr, ytr = tr[keep_feats], tr[cfg.TARGET_COL]\n",
    "        Xte, yte = te[keep_feats], te[cfg.TARGET_COL]\n",
    "\n",
    "        prep = {\"features\": keep_feats, \"strategy\": \"paper_drop\", \"rows_train\": len(tr), \"rows_test\": len(te)}\n",
    "        return Xtr, ytr, Xte, yte, prep\n",
    "\n",
    "    # For imputation strategies, optionally forward fill within ticker first\n",
    "    tr = train_df.copy()\n",
    "    te = test_df_eval.copy()\n",
    "\n",
    "    if cfg.MISSING_STRATEGY == \"ffill_then_median\":\n",
    "        if cfg.ID_COL in tr.columns:\n",
    "            tr = tr.sort_values([cfg.ID_COL, cfg.DATE_COL]).copy()\n",
    "            te = te.sort_values([cfg.ID_COL, cfg.DATE_COL]).copy()\n",
    "            tr[features] = tr.groupby(cfg.ID_COL)[features].ffill()\n",
    "            te[features] = te.groupby(cfg.ID_COL)[features].ffill()\n",
    "\n",
    "    # Fit imputer only on training\n",
    "    imputer = SimpleImputer(strategy=\"median\")\n",
    "    Xtr_raw = tr[features]\n",
    "    Xte_raw = te[features]\n",
    "\n",
    "    Xtr_imp = pd.DataFrame(imputer.fit_transform(Xtr_raw), columns=features, index=tr.index)\n",
    "    Xte_imp = pd.DataFrame(imputer.transform(Xte_raw), columns=features, index=te.index)\n",
    "\n",
    "    # Add missing indicators if requested\n",
    "    if cfg.ADD_MISSING_INDICATORS:\n",
    "        tr_miss = Xtr_raw.isna().astype(np.int8).add_prefix(\"miss__\")\n",
    "        te_miss = Xte_raw.isna().astype(np.int8).add_prefix(\"miss__\")\n",
    "        Xtr_imp = pd.concat([Xtr_imp, tr_miss], axis=1)\n",
    "        Xte_imp = pd.concat([Xte_imp, te_miss], axis=1)\n",
    "\n",
    "    ytr = tr[cfg.TARGET_COL].astype(float)\n",
    "    yte = te[cfg.TARGET_COL].astype(float)\n",
    "\n",
    "    prep = {\"features\": list(Xtr_imp.columns), \"strategy\": cfg.MISSING_STRATEGY, \"rows_train\": len(tr), \"rows_test\": len(te)}\n",
    "    return Xtr_imp, ytr, Xte_imp, yte, prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bffeaf",
   "metadata": {},
   "source": [
    "## 7) Time-respecting hyperparameter tuning (optional)\n",
    "\n",
    "The thesis uses a 10% bootstrap sample and 5-fold CV. The key fix here is: **do not shuffle time**.\n",
    "\n",
    "We implement a **purged, date-based CV**:\n",
    "- split by unique dates,\n",
    "- ensure the training fold ends at least `PURGE_MONTHS` before the validation fold (to avoid overlapping 1y-forward targets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a330d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def purged_date_splits(\n",
    "    df_train: pd.DataFrame,\n",
    "    cfg: Config,\n",
    "    n_splits: int = 5,\n",
    ") -> List[Tuple[np.ndarray, np.ndarray]]:\n",
    "    # split on unique dates, not rows\n",
    "    dates = pd.to_datetime(df_train[cfg.DATE_COL].unique())\n",
    "    dates = np.array(sorted(dates))\n",
    "\n",
    "    if len(dates) < (n_splits + 2):\n",
    "        return []\n",
    "\n",
    "    # Split dates into consecutive blocks for validation\n",
    "    folds = np.array_split(dates, n_splits + 1)  # last block can be unused buffer\n",
    "    splits = []\n",
    "    purge = pd.DateOffset(months=cfg.PURGE_MONTHS)\n",
    "\n",
    "    for i in range(n_splits):\n",
    "        val_dates = folds[i+1]  # skip first block so each split has some history\n",
    "        if len(val_dates) == 0:\n",
    "            continue\n",
    "        val_start = val_dates.min()\n",
    "\n",
    "        # Purge: training must end at least 12 months before val_start\n",
    "        train_end = val_start - purge\n",
    "        train_mask = df_train[cfg.DATE_COL] <= train_end\n",
    "        val_mask = df_train[cfg.DATE_COL].isin(val_dates)\n",
    "\n",
    "        tr_idx = df_train.index[train_mask].to_numpy()\n",
    "        va_idx = df_train.index[val_mask].to_numpy()\n",
    "        if len(tr_idx) and len(va_idx):\n",
    "            splits.append((tr_idx, va_idx))\n",
    "\n",
    "    return splits\n",
    "\n",
    "def tune_rf(\n",
    "    df_train_window: pd.DataFrame,\n",
    "    features: List[str],\n",
    "    cfg: Config,\n",
    "    param_grid: Dict,\n",
    "    n_iter: int = 25,\n",
    "    sample_frac: float = 0.10,\n",
    "    random_state: int = 42,\n",
    "):\n",
    "    # Bootstrap sample (paper uses 10% for grid search feasibility)\n",
    "    df_samp = df_train_window.sample(frac=min(sample_frac, 1.0), random_state=random_state)\n",
    "\n",
    "    # Prepare data using same missing strategy (paper_drop recommended for closest replication)\n",
    "    Xtr, ytr, _, _, prep = prep_Xy(df_samp, df_samp, features, cfg)\n",
    "    if len(Xtr) < 1000:\n",
    "        print(\"[WARN] Very small tuning sample after prep:\", len(Xtr))\n",
    "\n",
    "    splits = purged_date_splits(df_samp.dropna(subset=[cfg.TARGET_COL]), cfg, n_splits=5)\n",
    "    if not splits:\n",
    "        print(\"[WARN] Not enough unique dates for purged CV; skipping tuning.\")\n",
    "        return cfg.RF_PARAMS_PAPER\n",
    "\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    candidates = list(ParameterSampler(param_grid, n_iter=n_iter, random_state=rng))\n",
    "\n",
    "    best = None\n",
    "    best_mse = np.inf\n",
    "\n",
    "    # Map df index to positions in Xtr\n",
    "    idx_to_pos = {idx: pos for pos, idx in enumerate(Xtr.index.to_numpy())}\n",
    "\n",
    "    for p in candidates:\n",
    "        model = RandomForestRegressor(**{**cfg.RF_PARAMS_PAPER, **p})\n",
    "\n",
    "        fold_mses = []\n",
    "        for tr_idx, va_idx in splits:\n",
    "            tr_pos = [idx_to_pos[i] for i in tr_idx if i in idx_to_pos]\n",
    "            va_pos = [idx_to_pos[i] for i in va_idx if i in idx_to_pos]\n",
    "            if len(tr_pos) == 0 or len(va_pos) == 0:\n",
    "                continue\n",
    "\n",
    "            model.fit(Xtr.iloc[tr_pos], ytr.iloc[tr_pos])\n",
    "            pred = model.predict(Xtr.iloc[va_pos])\n",
    "            fold_mses.append(mean_squared_error(ytr.iloc[va_pos], pred))\n",
    "\n",
    "        if not fold_mses:\n",
    "            continue\n",
    "        mse = float(np.mean(fold_mses))\n",
    "        if mse < best_mse:\n",
    "            best_mse = mse\n",
    "            best = p\n",
    "\n",
    "    if best is None:\n",
    "        return cfg.RF_PARAMS_PAPER\n",
    "\n",
    "    tuned = {**cfg.RF_PARAMS_PAPER, **best}\n",
    "    print(\"Best CV MSE:\", best_mse, \"Best params delta:\", best)\n",
    "    return tuned\n",
    "\n",
    "# Example tuning grid (expanded vs v3; still practical)\n",
    "PARAM_GRID = {\n",
    "    \"max_depth\": list(range(3, 16)),\n",
    "    \"n_estimators\": list(range(100, 501, 50)),\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 5],\n",
    "    \"max_features\": [\"sqrt\", 0.33, 0.5],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974a4908",
   "metadata": {},
   "source": [
    "## 8) Walk-forward backtest (paper-faithful)\n",
    "\n",
    "For each test year Y:\n",
    "\n",
    "- `investment_date` = last month-end in Y (ideally 31/12/Y)\n",
    "- `train_end` = 31/12/(Y-1)\n",
    "- training data = all rows with `date <= train_end`\n",
    "- test data = all rows with `date == investment_date`\n",
    "\n",
    "We fit once per year and rank stocks by predicted 1-year return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e0521b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_mean_return(y_true: np.ndarray, y_pred: np.ndarray, n: int) -> float:\n",
    "    if len(y_pred) == 0:\n",
    "        return float(\"nan\")\n",
    "    n = min(n, len(y_pred))\n",
    "    idx = np.argsort(y_pred)[-n:]\n",
    "    return float(np.mean(y_true[idx]))\n",
    "\n",
    "def run_walk_forward_rf(\n",
    "    df: pd.DataFrame,\n",
    "    features: List[str],\n",
    "    cfg: Config,\n",
    "    rf_params: Optional[Dict] = None,\n",
    "    do_tune: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    rf_params = rf_params or cfg.RF_PARAMS_PAPER\n",
    "\n",
    "    df = df.copy()\n",
    "    df = apply_universe_filter(df, cfg)\n",
    "\n",
    "    first_y, last_y = auto_test_year_range(df, cfg)\n",
    "\n",
    "    rows = []\n",
    "    yearly_preds = []\n",
    "\n",
    "    for year in range(first_y, last_y + 1):\n",
    "        invest_date = last_month_end_in_year(df, cfg.DATE_COL, year)\n",
    "        if invest_date is None:\n",
    "            rows.append({\n",
    "                \"year\": year,\n",
    "                \"investment_date\": pd.NaT,\n",
    "                \"train_end\": year_end(year-1),\n",
    "                \"status\": \"SKIP_NO_DATE\",\n",
    "                \"features\": None,\n",
    "                \"strategy\": None,\n",
    "                \"rows_train\": None,\n",
    "                \"rows_test\": None,\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        train_end = year_end(year - 1)\n",
    "\n",
    "        df_train = df[df[cfg.DATE_COL] <= train_end].copy()\n",
    "        df_test = df[df[cfg.DATE_COL] == invest_date].copy()\n",
    "\n",
    "        if len(df_train) == 0 or len(df_test) == 0:\n",
    "            rows.append({\n",
    "                \"year\": year,\n",
    "                \"investment_date\": invest_date,\n",
    "                \"train_end\": train_end,\n",
    "                \"status\": \"SKIP_EMPTY_WINDOW\",\n",
    "                \"features\": None,\n",
    "                \"strategy\": None,\n",
    "                \"rows_train\": int(len(df_train)),\n",
    "                \"rows_test\": int(len(df_test)),\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Optional time-respecting tuning on this window\n",
    "        params_here = rf_params\n",
    "        if do_tune:\n",
    "            params_here = tune_rf(df_train, features, cfg, PARAM_GRID, n_iter=40, sample_frac=0.10)\n",
    "\n",
    "        Xtr, ytr, Xte, yte, prep = prep_Xy(df_train, df_test, features, cfg)\n",
    "        if len(Xtr) < 1000 or len(Xte) < 50:\n",
    "            rows.append({\n",
    "                \"year\": year, \"investment_date\": invest_date, \"train_end\": train_end,\n",
    "                \"status\": \"SKIP_SMALL\",\n",
    "                **prep\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        model = RandomForestRegressor(**params_here)\n",
    "        model.fit(Xtr, ytr)\n",
    "        pred = model.predict(Xte)\n",
    "\n",
    "        # Metrics aligned with portfolio use-case\n",
    "        mse = float(mean_squared_error(yte, pred))\n",
    "        rho, rho_p = spearmanr(pred, yte)  # ranking agreement\n",
    "\n",
    "        out = {\n",
    "            \"year\": year,\n",
    "            \"investment_date\": invest_date,\n",
    "            \"train_end\": train_end,\n",
    "            \"status\": \"OK\",\n",
    "            \"mse\": mse,\n",
    "            \"spearman_r\": float(rho),\n",
    "            \"spearman_p\": float(rho_p),\n",
    "            \"num_train\": int(len(Xtr)),\n",
    "            \"num_test_eval\": int(len(Xte)),\n",
    "            \"rows_train\": int(prep.get(\"rows_train\", len(Xtr))),\n",
    "            \"rows_test\": int(prep.get(\"rows_test\", len(Xte))),\n",
    "            \"strategy\": prep.get(\"strategy\"),\n",
    "            \"features\": prep.get(\"features\"),\n",
    "            \"num_features_used\": int(len(prep[\"features\"])),\n",
    "            \"missing_strategy\": prep[\"strategy\"],\n",
    "        }\n",
    "\n",
    "        # Portfolio returns (equal-weight top N)\n",
    "        for n in cfg.PORTFOLIO_SIZES:\n",
    "            out[f\"ret_top_{n}\"] = top_n_mean_return(np.asarray(yte), pred, n)\n",
    "\n",
    "        rows.append(out)\n",
    "\n",
    "        pred_frame = df_test.loc[Xte.index, [cfg.ID_COL, cfg.DATE_COL]].copy()\n",
    "        pred_frame[\"y_true\"] = yte.values\n",
    "        pred_frame[\"y_pred\"] = pred\n",
    "        pred_frame[\"rank_pred_desc\"] = (-pred_frame[\"y_pred\"]).rank(method=\"first\")\n",
    "        pred_frame[\"test_year\"] = year\n",
    "        yearly_preds.append(pred_frame)\n",
    "\n",
    "        print(f\"[OK] {year} invest={invest_date.date()} train_end={train_end.date()} \"\n",
    "              f\"train={len(Xtr)} test_eval={len(Xte)} feats={len(prep['features'])} mse={mse:.4f} rho={rho:.3f}\")\n",
    "\n",
    "    results = pd.DataFrame(rows)\n",
    "    if results.empty or (\"year\" not in results.columns):\n",
    "        print(\"[WARN] No results produced. Check date alignment, universe filters, and target availability.\")\n",
    "        # Return an empty, schema-stable DataFrame\n",
    "        results = pd.DataFrame(columns=[\n",
    "            \"year\",\"investment_date\",\"train_end\",\"status\",\"mse\",\"spearman_r\",\"spearman_p\",\n",
    "            \"num_train\",\"num_test_eval\",\"num_features_used\",\"prep_mode\",\"impute_mode\",\n",
    "            \"num_features_in\",\"num_features_dropped_missing\",\"num_rows_dropped_missing\"\n",
    "        ])\n",
    "        return results\n",
    "    results = results.sort_values(\"year\").reset_index(drop=True)\n",
    "\n",
    "    # Persist\n",
    "    results.to_csv(os.path.join(cfg.OUT_DIR, \"rf_walkforward_summary.csv\"), index=False)\n",
    "    if yearly_preds:\n",
    "        pd.concat(yearly_preds, ignore_index=True).to_csv(os.path.join(cfg.OUT_DIR, \"rf_yearly_predictions.csv\"), index=False)\n",
    "\n",
    "    # Store config snapshot\n",
    "    with open(os.path.join(cfg.OUT_DIR, \"run_config.json\"), \"w\") as f:\n",
    "        json.dump(asdict(cfg), f, default=str, indent=2)\n",
    "\n",
    "    return results\n",
    "\n",
    "summary = run_walk_forward_rf(df, ALL_FEATURES, CFG, rf_params=CFG.RF_PARAMS_PAPER, do_tune=False)\n",
    "display(summary.tail(10))\n",
    "print(\"Wrote:\", os.path.join(CFG.OUT_DIR, \"rf_walkforward_summary.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23f60f3",
   "metadata": {},
   "source": [
    "## 9) Performance aggregation (paper-style)\n",
    "\n",
    "We compute:\n",
    "- mean annual return for each portfolio size,\n",
    "- volatility (std),\n",
    "- excess return vs a benchmark (optional),\n",
    "- **SIR** (mean excess return / std excess return),\n",
    "- 1-tailed Welch\u2019s t-test vs benchmark (if provided).\n",
    "\n",
    "If you have a benchmark series, add it here. Otherwise, you can treat the market average return as a rough proxy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e31ee1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_portfolio_metrics(summary: pd.DataFrame, cfg: Config) -> pd.DataFrame:\n",
    "    ok = summary[summary[\"status\"] == \"OK\"].copy()\n",
    "    if ok.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    out_rows = []\n",
    "    for n in cfg.PORTFOLIO_SIZES:\n",
    "        r = ok[f\"ret_top_{n}\"].astype(float).dropna()\n",
    "        out_rows.append({\n",
    "            \"portfolio_n\": n,\n",
    "            \"years\": int(len(r)),\n",
    "            \"mean_return\": float(r.mean()),\n",
    "            \"std_return\": float(r.std(ddof=1)) if len(r) > 1 else float(\"nan\"),\n",
    "        })\n",
    "    return pd.DataFrame(out_rows)\n",
    "\n",
    "metrics = aggregate_portfolio_metrics(summary, CFG)\n",
    "display(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e613f44",
   "metadata": {},
   "source": [
    "## 10) Cumulative performance plots (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9225618",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_cumulative(summary: pd.DataFrame, cfg: Config, n: int):\n",
    "    ok = summary[summary[\"status\"] == \"OK\"].sort_values(\"year\").copy()\n",
    "    if ok.empty:\n",
    "        print(\"No OK rows.\")\n",
    "        return\n",
    "    rets = ok[f\"ret_top_{n}\"].astype(float).fillna(0.0).values\n",
    "    years = ok[\"year\"].values\n",
    "    cum = np.cumprod(1.0 + rets)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(years, cum)\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Cumulative growth (start=1.0)\")\n",
    "    plt.title(f\"RF top-{n} portfolio cumulative growth\")\n",
    "    plt.show()\n",
    "\n",
    "for n in CFG.PORTFOLIO_SIZES:\n",
    "    plot_cumulative(summary, CFG, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8120bfd7",
   "metadata": {},
   "source": [
    "## 11) Feature importance (averaged across test years)\n",
    "\n",
    "This reproduces the \u201cordered feature importances\u201d analysis from the thesis.\n",
    "For imputation strategies with missing indicators, importances will include `miss__*` features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77188e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_feature_importance(\n",
    "    df: pd.DataFrame,\n",
    "    features: List[str],\n",
    "    cfg: Config,\n",
    "    rf_params: Dict,\n",
    ") -> pd.DataFrame:\n",
    "    df = apply_universe_filter(df, cfg)\n",
    "    first_y, last_y = auto_test_year_range(df, cfg)\n",
    "\n",
    "    imp_rows = []\n",
    "    for year in range(first_y, last_y + 1):\n",
    "        invest_date = last_month_end_in_year(df, cfg.DATE_COL, year)\n",
    "        if invest_date is None:\n",
    "            continue\n",
    "        train_end = year_end(year - 1)\n",
    "        df_train = df[df[cfg.DATE_COL] <= train_end].copy()\n",
    "        df_test = df[df[cfg.DATE_COL] == invest_date].copy()\n",
    "\n",
    "        Xtr, ytr, Xte, yte, prep = prep_Xy(df_train, df_test, features, cfg)\n",
    "        if len(Xtr) < 1000:\n",
    "            continue\n",
    "\n",
    "        model = RandomForestRegressor(**rf_params)\n",
    "        model.fit(Xtr, ytr)\n",
    "\n",
    "        imps = pd.Series(model.feature_importances_, index=prep[\"features\"])\n",
    "        imps = imps.sort_values(ascending=False).head(50)\n",
    "        row = {\"year\": year, **{k: float(v) for k, v in imps.items()}}\n",
    "        imp_rows.append(row)\n",
    "\n",
    "    if not imp_rows:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    imp_df = pd.DataFrame(imp_rows).fillna(0.0)\n",
    "    mean_imp = imp_df.drop(columns=[\"year\"]).mean().sort_values(ascending=False)\n",
    "    out = pd.DataFrame({\"feature\": mean_imp.index, \"mean_importance\": mean_imp.values})\n",
    "    return out\n",
    "\n",
    "feat_imp = compute_feature_importance(df, ALL_FEATURES, CFG, CFG.RF_PARAMS_PAPER)\n",
    "display(feat_imp.head(30))\n",
    "feat_imp.to_csv(os.path.join(CFG.OUT_DIR, \"rf_feature_importance_mean.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5de332",
   "metadata": {},
   "source": [
    "## 12) Reproducing the paper\u2019s \u201cMid Cap and larger\u201d restriction\n",
    "\n",
    "Set `ALLOW_CAPS` and rerun. (This requires the panel to include a `cap` column.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112f0bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Mid Cap restriction (paper compares unrestricted vs Mid Cap+)\n",
    "CFG_MID = Config(**{**asdict(CFG), \"ALLOW_CAPS\": [\"Mid Cap\", \"Large Cap\", \"Mega Cap\"], \"OUT_DIR\": os.path.join(CFG.OUT_DIR, \"midcap_plus\")})\n",
    "os.makedirs(CFG_MID.OUT_DIR, exist_ok=True)\n",
    "\n",
    "summary_mid = run_walk_forward_rf(df, ALL_FEATURES, CFG_MID, rf_params=CFG_MID.RF_PARAMS_PAPER, do_tune=False)\n",
    "display(summary_mid.tail(10))\n",
    "\n",
    "metrics_mid = aggregate_portfolio_metrics(summary_mid, CFG_MID)\n",
    "display(metrics_mid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e49b02",
   "metadata": {},
   "source": [
    "## Notes on methodological fixes vs the thesis\n",
    "\n",
    "This notebook keeps the core experimental setup consistent with the thesis, but **fixes** two issues that can invalidate results:\n",
    "\n",
    "1) **No global target filtering**: target availability is handled per training/test window, preventing sample re-weighting.  \n",
    "2) **No random splits for tuning**: optional tuning uses a **date-based, purged CV** rather than shuffling.\n",
    "\n",
    "If you want *strict* thesis replication (including tuning procedure assumptions), set `do_tune=False` and use `RF_PARAMS_PAPER`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558d4655",
   "metadata": {},
   "source": [
    "## 13) Turnover and simple transaction-cost sensitivity (optional)\n",
    "\n",
    "The thesis uses an annual rebalance. Here we compute **turnover** as the fraction of names replaced year-to-year for each portfolio size, and optionally apply a **per-side cost in bps**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8113c233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_turnover(preds: pd.DataFrame, cfg: Config) -> pd.DataFrame:\n",
    "    # preds must include: test_year, TICKER, y_pred\n",
    "    out = []\n",
    "    for n in cfg.PORTFOLIO_SIZES:\n",
    "        prev = None\n",
    "        for year, g in preds.sort_values(\"test_year\").groupby(\"test_year\"):\n",
    "            top = g.nlargest(n, \"y_pred\")[cfg.ID_COL].astype(str).tolist()\n",
    "            top_set = set(top)\n",
    "            if prev is None:\n",
    "                out.append({\"portfolio_n\": n, \"year\": int(year), \"turnover\": np.nan})\n",
    "            else:\n",
    "                inter = len(top_set & prev)\n",
    "                turnover = 1.0 - (inter / float(n))\n",
    "                out.append({\"portfolio_n\": n, \"year\": int(year), \"turnover\": float(turnover)})\n",
    "            prev = top_set\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "preds_path = os.path.join(CFG.OUT_DIR, \"rf_yearly_predictions.csv\")\n",
    "if os.path.exists(preds_path):\n",
    "    preds = pd.read_csv(preds_path)\n",
    "    turnover = compute_turnover(preds, CFG)\n",
    "    display(turnover.tail(20))\n",
    "else:\n",
    "    print(\"No predictions file found:\", preds_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971984e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transaction_costs(returns: pd.Series, turnover: pd.Series, cost_bps_per_side: float = 10.0) -> pd.Series:\n",
    "    # Approx: annual rebalance; cost applied to traded notional.\n",
    "    # If turnover is fraction of names changed, traded notional roughly equals turnover (exit+enter).\n",
    "    # Multiply by 2 for both sides (sell+buy) unless you already define turnover as total traded.\n",
    "    cost = (cost_bps_per_side / 10000.0) * 2.0 * turnover.fillna(0.0)\n",
    "    return returns - cost\n",
    "\n",
    "# Example sensitivity (requires turnover computed above)\n",
    "if 'turnover' in locals() and not turnover.empty:\n",
    "    ok = summary[summary[\"status\"] == \"OK\"].copy()\n",
    "    merged = ok.merge(turnover, left_on=\"year\", right_on=\"year\", how=\"left\")\n",
    "    for n in CFG.PORTFOLIO_SIZES:\n",
    "        r = merged.loc[merged[\"portfolio_n\"] == n, f\"ret_top_{n}\"].astype(float)\n",
    "        t = merged.loc[merged[\"portfolio_n\"] == n, \"turnover\"].astype(float)\n",
    "        r_net = apply_transaction_costs(r, t, cost_bps_per_side=10.0)\n",
    "        print(f\"Top-{n}: mean gross={r.mean():.4f}, mean net@10bps/side={r_net.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e698aae",
   "metadata": {},
   "source": [
    "## 14) Baselines and diagnostics (optional)\n",
    "\n",
    "Because the end use-case is **ranking** and **portfolio selection**, MSE alone is not sufficient.\n",
    "\n",
    "Below are simple baselines to compare against:\n",
    "- **Market average**: mean 1-year return across all stocks in the investment universe on the investment date.\n",
    "- **Momentum proxy** (if a momentum feature exists): rank by that feature instead of RF prediction.\n",
    "\n",
    "You can extend this section with true benchmark index returns if you have an external series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6424bb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def market_average_return_on_date(df: pd.DataFrame, date: pd.Timestamp, cfg: Config) -> float:\n",
    "    x = df[df[cfg.DATE_COL] == date][cfg.TARGET_COL].dropna()\n",
    "    return float(x.mean()) if len(x) else float(\"nan\")\n",
    "\n",
    "# Compute market-average baseline for the same investment dates used in summary\n",
    "baseline = []\n",
    "for _, row in summary[summary[\"status\"]==\"OK\"].iterrows():\n",
    "    d = pd.to_datetime(row[\"investment_date\"])\n",
    "    baseline.append({\"year\": int(row[\"year\"]), \"market_avg_ret\": market_average_return_on_date(df, d, CFG)})\n",
    "\n",
    "baseline = pd.DataFrame(baseline)\n",
    "display(baseline.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e27c495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: compare RF top-N return vs market average baseline (not S&P 500)\n",
    "if not baseline.empty:\n",
    "    ok = summary[summary[\"status\"]==\"OK\"].merge(baseline, on=\"year\", how=\"left\")\n",
    "    for n in CFG.PORTFOLIO_SIZES:\n",
    "        er = ok[f\"ret_top_{n}\"].astype(float) - ok[\"market_avg_ret\"].astype(float)\n",
    "        sir = er.mean() / (er.std(ddof=1) if len(er)>1 else np.nan)\n",
    "        # One-tailed Welch t-test: H1 mean(RF) > mean(market_avg)\n",
    "        p = ttest_ind(ok[f\"ret_top_{n}\"].astype(float), ok[\"market_avg_ret\"].astype(float), equal_var=False, alternative=\"greater\").pvalue\n",
    "        print(f\"Top-{n}: mean_excess_vs_market_avg={er.mean():.4f}, SIR={sir:.3f}, one-tailed Welch p={p:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b7129b",
   "metadata": {},
   "source": [
    "## 15) Overfitting check for tuning (train vs validation MSE)\n",
    "\n",
    "If you enable tuning, it\u2019s useful to compare train vs validation error to spot overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ab89dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_train_val_mse_curve(df_train_window: pd.DataFrame, features: List[str], cfg: Config, param_list: List[Dict], sample_frac: float = 0.10):\n",
    "    df_samp = df_train_window.sample(frac=min(sample_frac, 1.0), random_state=42)\n",
    "    X, y, _, _, _ = prep_Xy(df_samp, df_samp, features, cfg)\n",
    "\n",
    "    splits = purged_date_splits(df_samp.dropna(subset=[cfg.TARGET_COL]), cfg, n_splits=5)\n",
    "    if not splits:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    idx_to_pos = {idx: pos for pos, idx in enumerate(X.index.to_numpy())}\n",
    "\n",
    "    rows=[]\n",
    "    for p in param_list:\n",
    "        model = RandomForestRegressor(**{**cfg.RF_PARAMS_PAPER, **p})\n",
    "        tr_mses=[]\n",
    "        va_mses=[]\n",
    "        for tr_idx, va_idx in splits:\n",
    "            tr_pos=[idx_to_pos[i] for i in tr_idx if i in idx_to_pos]\n",
    "            va_pos=[idx_to_pos[i] for i in va_idx if i in idx_to_pos]\n",
    "            if not tr_pos or not va_pos:\n",
    "                continue\n",
    "            model.fit(X.iloc[tr_pos], y.iloc[tr_pos])\n",
    "            pred_tr=model.predict(X.iloc[tr_pos])\n",
    "            pred_va=model.predict(X.iloc[va_pos])\n",
    "            tr_mses.append(mean_squared_error(y.iloc[tr_pos], pred_tr))\n",
    "            va_mses.append(mean_squared_error(y.iloc[va_pos], pred_va))\n",
    "        if tr_mses and va_mses:\n",
    "            rows.append({**p, \"train_mse\": float(np.mean(tr_mses)), \"val_mse\": float(np.mean(va_mses))})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Example curve varying n_estimators, keeping max_depth fixed\n",
    "# (Uncomment to run if needed)\n",
    "# curve_params = [{\"n_estimators\": n, \"max_depth\": 13} for n in range(50, 501, 50)]\n",
    "# curve = cv_train_val_mse_curve(df[df[CFG.DATE_COL] <= year_end(FIRST_Y-1)], ALL_FEATURES, CFG, curve_params, sample_frac=0.10)\n",
    "# display(curve)\n",
    "# if not curve.empty:\n",
    "#     plt.figure()\n",
    "#     plt.plot(curve[\"n_estimators\"], curve[\"train_mse\"])\n",
    "#     plt.plot(curve[\"n_estimators\"], curve[\"val_mse\"])\n",
    "#     plt.xlabel(\"n_estimators\")\n",
    "#     plt.ylabel(\"MSE\")\n",
    "#     plt.title(\"Train vs validation MSE (purged date CV)\")\n",
    "#     plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}